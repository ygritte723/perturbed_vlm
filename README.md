# Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination

## Overview

The approach enhances vision-language models by introducing text perturbations as negative samples and localized
attention representation for improving granularity level of understanding in biomedical applications.
<img src="readme/Per_arch.png" alt="Per_arch" title="Per_arch" width="700"/>

## Methodology

### Approach

The architecture integrates unimodal feature extractors for both images and texts, pretrained separately, and a
contrastive projection approach to fuse the cross-modal embeddings into a joint space. The novelty includes:

- **Local Attentive Contrastive Loss**: Boosts precision in medical Image-Text matching by aligning specific image
  sub-regions with relevant text fragments.
- **Report Perturbation Sensitivity Loss**: Enhances understanding of clinical semantics focusing on sentence structure
  and part of speech, paired with standard Image-Report Matching Contrastive Loss.

#### Text Perturbation Methods

The following table lists various text perturbation methods used in our approach:

| Perturbation Type                    | Description                                              |
|--------------------------------------|----------------------------------------------------------|
| Shuffle All Words                    | Randomly shuffles all words in a sentence                |
| Swap Adjacent Words                  | Swaps adjacent words in the sentence                     |
| Reverse Sentence                     | Reverses the order of words in a sentence                |
| Shuffle Within Trigrams              | Shuffles words within each trigram in the sentence       |
| Shuffle Trigrams                     | Shuffles the trigrams within the sentence                |
| Shuffle Nouns and Adjectives         | Shuffles nouns and adjectives while keeping others fixed |
| Shuffle All but Nouns and Adjectives | Shuffles all parts of speech except nouns and adjectives |
| Shuffle Nouns, Verbs, and Adjectives | Shuffles nouns, verbs, and adjectives only               |
| Replace Adjectives with Antonyms     | Replaces adjectives in the sentence with their antonyms  |

These perturbations are used to create negative samples for training the model, enhancing its ability to understand and
discriminate between perturbed and original reports.

## Installation

```bash
# Clone this repository
git clone https://github.com/ygritte723/perturbed_vlm.git
# Install dependencies
pip install -r requirements.txt
```

## Dataset

The model is trained on a curated dataset of biomedical images and associated reports. The dataset includes perturbed
reports generated by specific text manipulation rules.

- Open-I: Chest X-ray dataset with radiology reports
- RadNLI and MedNLI: Benchmarks containing labelled hypothesis and premise pairs
- CheXpert: Chest radiographs with associated radiology reports

## Evaluation

Our model demonstrated significant improvements in various biomedical vision-language tasks. Below are the summarized
results:

### Fine-Tuned Multi-task Image Classification on the CheXpert Benchmark

| Model                 | Accuracy on Consolidation (%) | Accuracy on Pleural Effusion (%) | Mean Accuracy (%) |
|-----------------------|-------------------------------|----------------------------------|-------------------|
| CLIP[3]               | 28.80                         | 43.60                            | 36.20             |
| GLoRIA[12]            | 71.11                         | 28.89                            | 50.00             |
| Ours (w/o local loss) | 33.80                         | 77.80                            | 55.80             |
| Ours                  | 93.40                         | 51.80                            | 72.60             |

### Fine-Tuned Text Classification on the RadNLI and MedNLI Benchmarks

| Model                 | MedNLI Accuracy (%) | RadNLI Accuracy (%) |
|-----------------------|---------------------|---------------------|
| CLIP[3]               | 86.80               | 68.50               |
| GLoRIA[12]            | 86.64               | 68.33               |
| Ours (w/o local loss) | 87.62               | 66.67               |
| Ours                  | 85.79               | 68.96               |

### Zero-Shot Clinical Semantic Structure Evaluation on the Open-I Benchmark

| Model                 | Open-I Accuracy (%) |
|-----------------------|---------------------|
| CLIP[3]               | 43.10               |
| GLoRIA[12]            | 44.30               |
| Ours (w/o local loss) | 46.30               |
| Ours                  | 49.00               |

