# Perturbed Report Discrimination for Biomedical Vision-Language Models

## Overview
The approach enhances vision-language models by introducing text perturbation techniques for improving semantic understanding in biomedical applications.
<img src="readme/Per_arch.png" alt="Per_arch" title="Per_arch" width="500"/>

## Methodology
### Approach
The model distinguishes between original biomedical reports and their perturbed versions, paired with images. 

#### Text Perturbation Methods
| Perturbation Type | Description |
| ----------------- | ----------- |
| Shuffle Words | Randomly shuffles words in a sentence |
| Reverse Sentence | Reverses the order of words in a sentence |
| ... | ... |


## Installation
```bash
# Clone this repository
git clone https://github.com/ygritte723/perturbed_vlm.git
# Install dependencies
pip install -r requirements.txt
```

## Dataset
The model is trained on a curated dataset of biomedical images and associated reports. The dataset includes perturbed reports generated by specific text manipulation rules.
- Open-I: Chest X-ray dataset with radiology reports
- RadNLI and MedNLI: Benchmarks containing labelled hypothesis and premise pairs
- CheXpert: Chest radiographs with associated radiology reports


## Evaluation
Our model demonstrated significant improvements in various biomedical vision-language tasks. Below are the summarized results:


### Fine-Tuned Multi-task Image Classification on the CheXpert Benchmark
| Model                         | Accuracy on Consolidation (%) | Accuracy on Pleural Effusion (%) | Mean Accuracy (%) |
|-------------------------------|-------------------------------|----------------------------------|-------------------|
| CLIP[3]                       | 28.80                         | 43.60                            | 36.20             |
| GLoRIA[12]                    | 71.11                         | 28.89                            | 50.00             |
| Ours (w/o local loss)         | 33.80                         | 77.80                            | 55.80             |
| Ours                          | 93.40                         | 51.80                            | 72.60             |


### Fine-Tuned Text Classification on the RadNLI and MedNLI Benchmarks
| Model                         | MedNLI Accuracy (%) | RadNLI Accuracy (%) |
|-------------------------------|---------------------|---------------------|
| CLIP[3]                       | 86.80               | 68.50               |
| GLoRIA[12]                    | 86.64               | 68.33               |
| Ours (w/o local loss)         | 87.62               | 66.67               |
| Ours                          | 85.79               | 68.96               |


### Zero-Shot Clinical Semantic Structure Evaluation on the Open-I Benchmark
| Model                         | Open-I Accuracy (%) |
|-------------------------------|---------------------|
| CLIP[3]                       | 43.10               |
| GLoRIA[12]                    | 44.30               |
| Ours (w/o local loss)         | 46.30               |
| Ours                          | 49.00               |

