# -*- coding: utf-8 -*-
"""0810.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P-S4e69LVF_-6QSLw6xODx6-675InxlL

"""

"""# Environment preparation"""

# gpu_info = !nvidia-smi -i 0
# gpu_info = '\n'.join(gpu_info)
# print(gpu_info)
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
# test_device = torch.device('cuda:1')   # Change to the desired GPU index

"""# Set arguments"""

import argparse
import json
import math
import os
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import wandb
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from health_multimodal.image.data.io import load_image
from health_multimodal.image.data.transforms import (
    create_chest_xray_transform_for_inference,
)
from health_multimodal.image.model import ImageModel
from health_multimodal.image.model.pretrained import (
    BIOMED_VLP_CXR_BERT_SPECIALIZED,
    CXR_BERT_COMMIT_TAG,
)
from health_multimodal.image.model.types import ImageEncoderType
from health_multimodal.text.model import CXRBertModel, CXRBertTokenizer

# from health_multimodal.image.model.pretrained import get_imagenet_init_encoder
# from health_multimodal.image.model.resnet import resnet50
# from health_multimodal.text.inference_engine import TextInferenceEngine
# from enum import Enum, unique


parser = argparse.ArgumentParser(description="Train MoCo+BIOVIL")

parser.add_argument("-a", "--arch", default="resnet18")

# lr: 0.06 for batch 512 (or 0.03 for batch 256)
parser.add_argument(
    "--lr",
    "--learning-rate",
    default=0.0015,
    type=float,
    metavar="LR",
    help="initial learning rate",
    dest="lr",
)
parser.add_argument(
    "--epochs", default=100, type=int, metavar="N", help="number of total epochs to run"
)
parser.add_argument(
    "--schedule",
    default=[120, 160],
    nargs="*",
    type=int,
    help="learning rate schedule (when to drop lr by 10x); does not take effect if --cos is on",
)
parser.add_argument("--cos", action="store_true", help="use cosine lr schedule")

parser.add_argument(
    "--batch-size", default=64, type=int, metavar="N", help="mini-batch size"
)
parser.add_argument("--wd", default=5e-4, type=float, metavar="W", help="weight decay")

# moco specific configs:
parser.add_argument("--moco-dim", default=128, type=int, help="feature dimension")
parser.add_argument(
    "--moco-k", default=4096, type=int, help="queue size; number of negative keys"
)
parser.add_argument(
    "--moco-m", default=0.99, type=float, help="moco momentum of updating key encoder"
)
parser.add_argument("--moco-t", default=0.1, type=float, help="softmax temperature")

# parser.add_argument('--bn-splits', default=8, type=int, help='simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu')

# parser.add_argument('--symmetric', action='store_true', help='use a symmetric loss function that backprops to both crops')

# knn monitor
parser.add_argument("--knn-k", default=200, type=int, help="k in kNN monitor")
parser.add_argument(
    "--knn-t",
    default=0.1,
    type=float,
    help="softmax temperature in kNN monitor; could be different with moco-t",
)

# utils
parser.add_argument(
    "--resume",
    default="",
    type=str,
    metavar="PATH",
    help="path to latest checkpoint (default: none)",
)
parser.add_argument(
    "--results-dir",
    default="",
    type=str,
    metavar="PATH",
    help="path to cache (default: none)",
)
parser.add_argument("--wandb", default=False, type=bool, help="whether to use wandb")


args = parser.parse_args()  # running in command line

# args = parser.parse_args('')  # running in ipynb

# set command line arguments here when running in ipynb
args.epochs = 150
args.cos = True
args.schedule = []  # cos in use
# args.symmetric = False
if args.results_dir == "":
    args.results_dir = "./caches/cache-" + datetime.now().strftime(
        "%Y-%m-%d-%H-%M-%S-moco"
    )

print(args)

# ðŸ 1ï¸âƒ£ Start a new run to track this script
if args.wandb:
    wandb.login()
    wandb.init(
        # Set the project where this run will be logged
        project="0810_v2",
        # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)
        name="experiment_" + datetime.now().strftime("%Y-%m-%d-%H-%M-%S-moco"),
        # Track hyperparameters and run metadata
        config=args,
    )


"""# Dataloader"""
#  https://www.kaggle.com/code/ahmed010abdo178/gpt-vit2/notebook
# df2 = pd.read_csv('/ocean/projects/asc170022p/lisun/xinliu/images/csv/indiana_projections.csv')
# df1 = pd.read_csv('/ocean/projects/asc170022p/lisun/xinliu/images/csv/indiana_reports.csv')
# images_captions_df = pd.DataFrame({'image': [],'caption': []})
# for i in range(len(df2)):
#    uid = df2.iloc[i]['uid']
#    image = df2.iloc[i]['filename']
#    index = df1.loc[df1['uid'] == uid]

#    if not index.empty:
#        index = index.index[0]
#        caption = df1.iloc[index]['findings']
#        if type(caption) == float:
#            continue

#        images_captions_df = pd.concat([images_captions_df, pd.DataFrame([{'image': image, 'caption': caption}])], ignore_index=True)
# images_captions_df.head()
images_captions_df = pd.read_csv(
    "/ocean/projects/asc170022p/lisun/xinliu/images/csv/indiana_captions.csv"
)

# import swifter
new_df = images_captions_df.copy()
new_df = new_df.drop(index=range(6000, 6469))

val_df = images_captions_df.copy()
val_df = val_df.drop(index=range(0, 6000))
# print(val_df.head())


def collate_fn(batch):
    input_ids_batch = [item[0] for item in batch]
    attention_mask_batch = [item[1] for item in batch]
    image_input_batch = [item[2] for item in batch]

    # Pad sequences to the length of the longest sequence in the batch
    input_ids_padded = pad_sequence(input_ids_batch, batch_first=True)
    attention_mask_padded = pad_sequence(attention_mask_batch, batch_first=True)

    # Convert lists to tensors
    input_ids_tensor = input_ids_padded
    attention_mask_tensor = attention_mask_padded
    image_input_tensor = torch.stack(image_input_batch)
    # print(input_ids_tensor.shape, attention_mask_tensor.shape, image_input_tensor.shape)

    return input_ids_tensor, attention_mask_tensor, image_input_tensor


class OpenIDataset(Dataset):
    def __init__(self, df, root_dir, transform=None):
        self.df = df
        self.transform = transform
        self.root_dir = root_dir
        self.tokenizer = CXRBertTokenizer.from_pretrained(
            BIOMED_VLP_CXR_BERT_SPECIALIZED, revision=CXR_BERT_COMMIT_TAG
        )

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Text encoding (Bag of Words)
        caption = [self.df.caption.iloc[idx]]
        # print(caption,len(caption))

        tokenizer_output = self.tokenizer.batch_encode_plus(
            batch_text_or_text_pairs=caption,
            add_special_tokens=True,
            padding="longest",
            return_tensors="pt",
        )
        input_ids = tokenizer_output.input_ids.view(-1).squeeze()
        attention_mask = tokenizer_output.attention_mask.view(-1).squeeze()
        image = self.df.image.iloc[idx]
        img_path = os.path.join(self.root_dir, image)
        # print(idx)
        img_path = Path(img_path)
        img = load_image(img_path)

        if self.transform is not None:
            img = self.transform(img)

        # pixel_values = self.feature_extractor(img, return_tensors="pt").pixel_values
        # print(input_ids.shape, attention_mask.shape, img.shape)
        return input_ids, attention_mask, img


# train_dataset = OpenIDataset(
# new_df,
# root_dir= "/ocean/projects/asc170022p/lisun/xinliu/images/images_normalized"
# tokenizer=CXRBertTokenizer.from_pretrained(BIOMED_VLP_CXR_BERT_SPECIALIZED, revision=CXR_BERT_COMMIT_TAG)
# )
# print(train_dataset[0][0].shape, train_dataset[0][1].shape, train_dataset[0][2])
# gc.collect()
# torch.cuda.empty_cache()
# del train_dataset


transforms = create_chest_xray_transform_for_inference(
    # resize=512,
    # center_crop_size=448
    resize=256,
    center_crop_size=224,
)

train_dataset = OpenIDataset(
    new_df,
    root_dir="/ocean/projects/asc170022p/lisun/xinliu/images/images_normalized",
    # tokenizer=CXRBertTokenizer.from_pretrained(BIOMED_VLP_CXR_BERT_SPECIALIZED, revision=CXR_BERT_COMMIT_TAG),
    transform=transforms,
)
memory_dataset = OpenIDataset(
    new_df,
    root_dir="/ocean/projects/asc170022p/lisun/xinliu/images/images_normalized",
    # tokenizer=CXRBertTokenizer.from_pretrained(BIOMED_VLP_CXR_BERT_SPECIALIZED, revision=CXR_BERT_COMMIT_TAG),
    transform=transforms,
)
test_dataset = OpenIDataset(
    val_df,
    root_dir="/ocean/projects/asc170022p/lisun/xinliu/images/images_normalized",
    # tokenizer=CXRBertTokenizer.from_pretrained(BIOMED_VLP_CXR_BERT_SPECIALIZED, revision=CXR_BERT_COMMIT_TAG),
    transform=transforms,
)
train_loader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    shuffle=True,
    num_workers=2,
    pin_memory=False,
    drop_last=True,
    collate_fn=collate_fn,
)
memory_loader = DataLoader(
    memory_dataset,
    batch_size=args.batch_size,
    shuffle=False,
    num_workers=2,
    pin_memory=False,
    collate_fn=collate_fn,
)
test_loader = DataLoader(
    test_dataset,
    batch_size=args.batch_size,
    shuffle=False,
    num_workers=2,
    pin_memory=False,
    collate_fn=collate_fn,
)

print(train_dataset[0][1].shape)
"""# Contrastive model"""


class ContrastiveModel(nn.Module):
    def __init__(self, K=65536, m=0.999, T=0.07):
        super(ContrastiveModel, self).__init__()

        self.K = K
        self.m = m
        self.T = T
        # Text Encoder CXRBert
        self.text_encoder = CXRBertModel.from_pretrained(
            BIOMED_VLP_CXR_BERT_SPECIALIZED, revision=CXR_BERT_COMMIT_TAG
        )
        # Image Encoder ResNet50
        self.image_encoder = ImageModel(
            img_encoder_type=ImageEncoderType.RESNET50.value, joint_feature_size=128
        )

        self.criterion = nn.CrossEntropyLoss()

        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def contrastive_loss(self, input_ids, attention_mask, image_input):
        # print('image_input:', image_input.shape, image_input.min(), image_input.max())
        batch_size = input_ids.shape[0]

        # Text encoding
        text_embeds = self.text_encoder.get_projected_text_embeddings(
            input_ids=input_ids, attention_mask=attention_mask
        )

        # Image encoding
        image_embeds = self.image_encoder(image_input).projected_global_embedding

        text_embeds = nn.functional.normalize(text_embeds, dim=1)
        image_embeds = nn.functional.normalize(image_embeds, dim=1)

        logits_text_per_image = self.logit_scale.exp() * image_embeds @ text_embeds.t()
        logits_image_per_text = logits_text_per_image.t()

        target = (
            torch.arange(batch_size).long().to(text_embeds.device, non_blocking=True)
        )
        # print(text_embeds.device, image_embeds.device, self.device, logits_text_per_image.device)
        loss = (
            self.criterion(logits_text_per_image, target)
            + self.criterion(logits_image_per_text, target)
        ) / 2

        return loss

    def forward(self, input_ids, attention_mask, image_input):
        loss = self.contrastive_loss(input_ids, attention_mask, image_input)

        return loss


# Create the contrastive model

# print(model.text_encoder)
# print(model.image_encoder)
# model = torch.nn.DataParallel(model, device_ids=device_ids)

# Sample inputs
# text_input = ["There is no pneumothorax or pleural effusion",
# "The extent of the pleural effusion is constant."] # Example text input (replace with your own)
# image_input = torch.randn(2, 3, 128, 128).cuda(non_blocking=True)  # Example image batch with 2 images


# output = model(text_input, image_input)
# print("Output shape:", output.shape)
# print(output)

"""# Define train/test"""


# train for one epoch
def train(net, data_loader, train_optimizer, epoch, args):
    device = torch.device("cuda:0")  # Change to the desired GPU index
    net = net.to(device)
    net.train()
    adjust_learning_rate(train_optimizer, epoch, args)

    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)
    for input_ids, attention_mask, image_input in train_bar:
        input_ids, attention_mask, image_input = (
            input_ids.to(device),
            attention_mask.to(device),
            image_input.to(device),
        )
        # print(input_ids.shape, attention_mask.shape, image_input.shape)
        loss = net(input_ids, attention_mask, image_input)

        train_optimizer.zero_grad()
        loss.backward()
        train_optimizer.step()
        # torch.cuda.empty_cache()

        total_num += data_loader.batch_size
        total_loss += loss.item() * data_loader.batch_size
        train_bar.set_description(
            "Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}".format(
                epoch,
                args.epochs,
                train_optimizer.param_groups[0]["lr"],
                total_loss / total_num,
            )
        )

    return total_loss / total_num


# lr scheduler for training
def adjust_learning_rate(optimizer, epoch, args):
    """Decay the learning rate based on schedule"""
    lr = args.lr
    if args.cos:  # cosine lr schedule
        lr *= 0.5 * (1.0 + math.cos(math.pi * epoch / args.epochs))
    else:  # stepwise lr schedule
        for milestone in args.schedule:
            lr *= 0.1 if epoch >= milestone else 1.0
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# test using a knn monitor

"""# Start training"""


# define optimizer
def main():
    device = torch.device("cuda:0")
    model = ContrastiveModel().to(device)
    model.train()

    optimizer = torch.optim.SGD(
        model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9
    )

    # load model if resume
    epoch_start = 1
    if args.resume != "":
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint["state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer"])
        epoch_start = checkpoint["epoch"] + 1
        print("Loaded from: {}".format(args.resume))

    # logging
    # results = {'train_loss': [], 'test_acc@1': []}
    results = {"train_loss": []}
    if not os.path.exists(args.results_dir):
        os.mkdir(args.results_dir)
    # dump args
    with open(args.results_dir + "/args.json", "w") as fid:
        json.dump(args.__dict__, fid, indent=2)

    # training loop
    for epoch in range(epoch_start, args.epochs + 1):
        train_loss = train(model, train_loader, optimizer, epoch, args)
        results["train_loss"].append(train_loss)
        # test_acc_1 = test(model, memory_loader, test_loader, epoch, args)
        # results['test_acc@1'].append(test_acc_1)
        # save statistics
        data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))
        data_frame.to_csv(args.results_dir + "/log.csv", index_label="epoch")
        # save model
        torch.save(
            {
                "epoch": epoch,
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
            },
            args.results_dir + "/model_last.pth",
        )
        if args.wandb:
            wandb.log({"loss": train_loss})
        # gc.collect()
        # torch.cuda.empty_cache()
        # model.save_pretrained("./save_pretrained/"+str(epoch))
    if args.wandb:
        wandb.finish()


if __name__ == "__main__":
    main()
